---
title: "insurance analysis"
author: "Alex Fennell"
output: html_document
---



```{r libraries, message=FALSE,warning=FALSE,echo=FALSE}
library(tidyverse)
library(tidymodels)
library(psych)
library(plotly)
library(gridExtra)
library(doParallel)
```

```{r read in data}
data<-read.csv("train.csv")
test<-read.csv("test.csv")
```

# Data Preparation
Most of the variables are not of the proper type, so I will convert character variables
to factors and recode some of them to have meaningful levels.

```{r convert to factor}
data<-data%>%
    mutate_if(is.character,as.factor)%>%
    mutate_at(c("Driving_License","Previously_Insured","Response","Region_Code",
                "Policy_Sales_Channel"),as.factor)

data$Response<-recode_factor(data$Response, `0` = "No",
                              `1` = "Yes")
data$Vehicle_Damage<-recode_factor(data$Vehicle_Damage,No="No Damage",
                                    Yes="Damage")
data$Previously_Insured<-recode_factor(data$Previously_Insured,`0`="No Insurance",
                                        `1`="Insurance")
data$Driving_License<-recode_factor(data$Driving_License,`0`="No DL",
                                     `1`="Has DL")

test<-test%>%
    mutate_if(is.character,as.factor)%>%
    mutate_at(c("Driving_License","Previously_Insured","Region_Code",
                "Policy_Sales_Channel"),as.factor)

test$Vehicle_Damage<-recode_factor(test$Vehicle_Damage,No="No Damage",
                                    Yes="Damage")
test$Previously_Insured<-recode_factor(test$Previously_Insured,`0`="No Insurance",
                                        `1`="Insurance")
test$Driving_License<-recode_factor(test$Driving_License,`0`="No DL",
                                     `1`="Has DL")
```
## Create validation/training data sets
Since this particular set of data already has a separate csv file for the test data
set, I will split the training dataset to create a validation set. So as to be able
to test the models and avoid overfitting.

``` {r data split}
set.seed(1111)
split<-initial_split(data,prop=.7,strata=Response)
train<-training(split)
valid<-testing(split)

```


#Exploratory Data Analysis

```{r data summary}
summary(train)
```

This summary shows a few things of interest. First is that the outcome variable
is quite unbalanced with the majority of individuals being not interested (~88%).
This can have an effect on model performance, and various sampling techniques (e.g. upsample/smote)
will be implemented. The Driving_license predictor is also very unbalanced with a tiny
subset of individuals having no driver's license, thus this predictor may not be very
useful. There are also a relatively small proportion of individuals in this sample with
a car age greater than 2 years. The other factor variables seem to have a relatively
even balance of responses in their different levels. The numeric variables such as 
age seem to be within a reasonable range.

## Histograms of Numeric Predictors
```{r univariate numeric analysis,warning=FALSE,message=FALSE}
agep<-ggplotly(qplot(train$Age))
premiump<-ggplotly(qplot(train$Annual_Premium))
vintp<-ggplotly(qplot(train$Vintage))

subplot(agep,premiump,vintp,margin=.06,nrows=1)%>%
    layout(annotations=list(
        list(x=.1,y=1.05,text="Age",showarrow = FALSE, xref='paper', yref='paper'),
        list(x=.5,y=1.05,text="Annual Premium",showarrow = FALSE, xref='paper', yref='paper'),
        list(x=.95,y=1.05,text="Number of Days with Company",showarrow = FALSE, xref='paper', yref='paper')
    ))
```

This plot shows that age has a skewed bimodal distribution with the majority of individuals
being around 25 years of age, and another concentration of individuals around 40 years of age.
The annual premium is also skewed but this is likely due to an outlier. As was seen
in the summary there is a maximum annual premium of 540,165, which is far above the
median value of 31,662. The number of days with the company is an almost uniformly
distributed predictor.

## Categorical Bar Charts
### Insurance Expansion as a function of vehicle age and current Insurance status

```{r categorical variables}
catp1<-ggplot(data=train,aes(x=Response,fill=Vehicle_Age))+
    geom_bar(position="dodge")+
    facet_wrap(~Previously_Insured)
ggplotly(catp1)
```
### Insurance expansion as a function of gender and driver's license possesion

```{r categorical variables 2}
catp2<-ggplot(data=train,aes(x=Response,fill=Gender))+
    geom_bar(position="dodge")+
    facet_wrap(~Driving_License)
ggplotly(catp2)
```

### Boxplots of insurance expansion as a function of vehicle age and current insurance status
```{r boxplots,warning=FALSE}
bp1<-ggplot(data=train,aes(x=Response,y=Age,fill=Vehicle_Age))+
    geom_boxplot(position=position_dodge(1))+
    facet_wrap(~Previously_Insured)
ggplotly(bp1)%>%
    layout(boxmode="group")

```

# Model preparation

For this data set I am going to be comparing 3 different models. Since my outcome is
binary I will start with a basic logistic regression model. Then I will use a penalized
logistic regression model and finally a random forest model. To prepare the data for modelling
I first update the role of the ID variable so that it is included in the dataset
but is not used in the modelling. Then I use step other to clump together categories in 
factors that have few observations (less than 5% of the data). This will help reduce
the dimensionality of the data. I scale the data (subtract each value from the mean
and divide by the SD) to make sure all predictors are on the same scale so large values
don't incorrectly bias parameter estimates. Then the logistic regression models require the 
factor predictors to be converted to dummy variables. Finally I use step nsv to remove
any predictors that are highly unbalanced such as the driver's license predictor. Finally, I
create cross-validation folds which will be used to further reduce the likelihood of
the models overfitting.

```{r prepare for modelling,warning=FALSE,message=FALSE}
levels(train$Response)<-c("No","Yes")
insurance_recipe<-recipe(Response~.,data=train)%>%
    update_role(id,new_role = "ID")%>%
    step_other(all_nominal_predictors())%>%
    step_scale(all_numeric_predictors())%>%
    step_dummy(all_nominal_predictors())%>%
    step_nzv(all_predictors(),freq_cut = 99/1)

# create cross validation sets to reduce model variance
cv_folds<-vfold_cv(train,v=5)
```

## Setting up model engines and hyperparameter tuning
```{r creating model specifications}

lr_mod<-logistic_reg()%>%
    set_mode("classification")%>%
    set_engine("glm")

lr_mod_RL<-logistic_reg(penalty=tune(),mixture=tune())%>%
    set_engine("glmnet")%>%
    set_mode("classification")

rf_mod<-rand_forest(mtry=tune(),trees=tune())%>%
    set_mode("classification")%>%
    set_engine("ranger")
    
```


## Specifying hyperparameter tuning grid
For the hyperparameter tuning grid I am letting the tune function specify the range
for the penalized logistic regression. This tidymodels function selects reasonable
endpoints for each parameter. For example, the mixture parameter has a range from 0-1
with 0 indicating a fully ridge model, and 1 indicating a fully lasso model. For the
random forest model I specify the hyperparameter endpoints as these are the values I
am most interested in testing.

```{r specify tuning grid}
lr_grid<-grid_regular(penalty(),
                      mixture(),
                      levels=7)
rf_grid<-grid_regular(mtry()%>%range_set(c(1, 10)),
                      trees()%>%range_set(c(500,2000)),
                      levels=4)
```

### Making workflow to allow easy interchange of the models
```{r create workflow}
insurance_wf<-workflow() %>%
    add_recipe(insurance_recipe)%>%
    add_model(lr_mod)
```

# Model Fitting
## Logistic Regression Model Fit
For the model fits I am collecting 3 different metrics F-score, Accuracy and the
roc_auc score. These are three common metrics with AUC being the metric that I am
particularly interested in.

```{r Logistic Regression Model fit,warning=FALSE,message=FALSE,eval=FALSE}
lr_fit_train<- insurance_wf%>%
    fit_resamples(resamples=cv_folds,
                  metrics = metric_set(
                      f_meas,
                      accuracy,
                      roc_auc),
                  control = control_resamples(save_pred = TRUE))

saveRDS(lr_fit_train,"lr_basicfit.rds")
```

##Penalized Logistic Regression Model fit-hyperparameter tuning

```{r ridge regression model,warning=FALSE,message=FALSE,eval=FALSE}
insurance_wf<-update_model(insurance_wf,lr_mod_RL)
lr_pen_fit<-insurance_wf%>%
    tune_grid(resamples=cv_folds,
                  grid=lr_grid,
                      metrics = metric_set(
                      f_meas,
                      accuracy,
                      roc_auc),
                  control=control_grid(save_pred=TRUE))
saveRDS(lr_pen_fit,"lr_pen_fit.rds")

```

```{r read in Ridge/Lasso Regression Model,echo=FALSE}
lr_fit_train<-readRDS("lr_basicfit.rds")
lr_pen_fit<-readRDS("lr_pen_fit.rds")
RF_fit<-readRDS("RF_mod_Grid.rds")

```

## Penalized Regression Hyperparameter fit evaluation
```{r Ridge/Lasso Regression hyperparamter evaluation}
lr_pen_fit %>%
  collect_metrics() %>%
  mutate(mixture = factor(mixture)) %>%
  filter(mean>.8)%>%
  ggplot(aes(penalty, mean, color = mixture)) +
  geom_line(size = 1.2, alpha = 0.5) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free_y", nrow = 2) +
  scale_x_log10(labels = scales::scientific,limits=c(NA,.05)) +
  scale_color_viridis_d(begin = .9, end = 0)

```

This set of plots indicates that a mixture value of zero seems to provide the best
fitting model across all penalty values for accuracy and the F-measure. Mixture values other than
zero achieve a similar level of accuracy and F-score when the penalty is large. In relation 
to the AUC value, the model with a mixture a value of 0 does worse until the penalty 
value is relatively large, and then it is the best. That being said the difference in
performance between these hyperparameter values is very small, differing by values in
the ten thousandths for the most part. Since AUC is the main metric of interest, I will
put more weight on that in making my modelling choice.

```{r best penalized logistic regression}
lr_pen_best<-select_best(lr_pen_fit,metric = "roc_auc")
show_best(lr_pen_fit,metric="roc_auc")
```


This function shows that the best model based on AUC is one with a mixture of 1/6
and a penalty of 1e^-10 which will be the penalized logistic regression model I use
going forward.

## Random Forest Model Fit
Although the random forest model does not require dummy variables I will still use
the same recipe as the logistic regression for the sake of simplicity.

```{r random forest model,warning=FALSE,message=FALSE,eval=FALSE}
set.seed(1234)
insurance_wf<-update_model(insurance_wf,rf_mod)
RF_fit<-insurance_wf%>%
    tune_grid(resamples=cv_folds,
                  grid=rf_grid,
                  metrics = metric_set(
                      f_meas,
                      accuracy,
                      roc_auc),
                  control=control_grid(save_pred = TRUE)
              )
saveRDS(RF_fit,"RF_mod_Grid.rds")

```

## Random Forest Hyperparameter fit evaluation

```{r RF model hyperparamter visualization}
RF_fit %>%
  collect_metrics() %>%
  mutate(mtry = factor(mtry)) %>%
  ggplot(aes(trees, mean, color = mtry)) +
  geom_line(size = 1.2, alpha = 0.5) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_color_viridis_d(begin=.9,end=0)

```
This plot shows how performance metrics change as a function of the number of trees
and the number of randomly sampled predictors at each split. Across all metrics an
mtry value of 7 or 10 performs the worst and is relatively unaffected by the number
of trees. mtry of 1 and 4 are nearly equivalent for accuracy and the f1-score with the 
mtry of 1 possibly being slightly better. With AUC, mtry of 4 consistently outperforms
the other mtry possibilities. The number of trees seems to have little influence
on the model performance, with a slight increase from 500-1000 trees but above that
seems to offer no benefit. The table below will show the best parameter combinations
based on AUC.

```{r RF best model selection}
rf_best_fit<-select_best(RF_fit,metric="roc_auc")
show_best(RF_fit,metric="roc_auc")
```


# Model comparison
we will begin our model comparison by examining the confusion matrices and then
examine the ROC curve.

```{r collect data for confusion matrices}
lr_pred<-
    lr_fit_train%>%
    collect_predictions()

lr_pen_pred<-
    lr_fit_train%>%
    collect_predictions(parameters=lr_pen_best)

rf_pred<-
    RF_fit%>%
    collect_predictions(parameters=rf_best_fit)
```

## Confusion Matrices
```{r}
cm_lr<-lr_pred%>%
    conf_mat(Response,.pred_class)%>%
    autoplot(type='heatmap')+
    ggtitle("Logistic Regression")
cm_lr_pen<-lr_pen_pred%>%
    conf_mat(Response,.pred_class)%>%
    autoplot(type='heatmap')+
    ggtitle("Penalized Regression")
cm_rf<-rf_pred%>%
    conf_mat(Response,.pred_class)%>%
    autoplot(type='heatmap')+
    ggtitle("Random Forest")
    
grid.arrange(cm_lr,cm_lr_pen,cm_rf,ncol=3,heights=c(1,1))
```

This plot of confusion matrices demonstrates a huge problem across all the models.
The models are quite good at capturing true positives (a predicted "No" response
in agreement with the actual data) but fails to capture the true negatives (a predicted
"Yes" response in agreement with the actual data). All the models make an abundance
of false positives (a predicted "No" response when the data is a "Yes" response)
which is quite problematic since I am interested in developing a model that can
capture the "Yes" responses. This is due to the unbalanced nature of the outcome
variable. To address this I will implement upsampling, downsampling and smote sampling
to find a method to address this. I will examine the specificity and ROC values of
the already fit models to determine which model I will test these different sampling methods with
first. 

```{r collect specificity and ROC values}
lr_spec<-
    lr_pred%>%
    spec(Response,.pred_class)%>%
    mutate(Model="Logistic Regression")
lr_pen_spec<-
    lr_pen_pred%>%
    spec(Response,.pred_class)%>%
    mutate(Model="Penalized Logistic Regression")
rf_spec<-
    rf_pred %>%
    spec(Response,.pred_class)%>%
    mutate(Model="Random Forest")

lr_auc<-
    lr_pred%>%
    roc_auc(Response,.pred_No)%>%
    mutate(Model="Logistic Regression")
lr_pen_auc<-
    lr_pen_pred%>%
    roc_auc(Response,.pred_No)%>%
    mutate(Model="Penalized Regression")
rf_auc<-
    rf_pred%>%
    roc_auc(Response,.pred_No)%>%
    mutate(Model="Random Forest")
    
```

```{r Compare Specificity and ROC}
spec_plot<-bind_rows(lr_spec,lr_pen_spec,rf_spec) %>% 
  ggplot(aes(x=.estimate,y = Model, fill = as.factor(Model))) + 
  geom_col()+
  scale_fill_brewer(type="qual",palette="Dark2")+
  labs(title="Specificity Across Models",x="Specificity",fill="Model")+
  theme(plot.title = element_text(hjust=.5))

auc_plot<-bind_rows(lr_auc,lr_pen_auc,rf_auc) %>% 
  ggplot(aes(x=.estimate,y = Model, fill = as.factor(Model))) + 
  geom_col()+
  scale_fill_brewer(type="qual",palette="Dark2")+
  labs(title="AUC Across Models",x="AUC",fill="Model")+
  theme(plot.title = element_text(hjust=.5))+
  geom_text(
     size = 3,
     aes(label = round(.estimate,2), y=Model,x = .estimate + 0.01),
     vjust = 1, hjust=.15
  )

grid.arrange(spec_plot,auc_plot,nrow=2)
```


These plots show that the random forest model has both a slightly better AUC and
specificity than the other models. However given the additional complexity of the
random forest model, the increase in performance is negligible. Therefore, I will
be using the logistic regression model to test whether upsampling, downsampling or
smote sampling will best address the class imbalance.

# Class Imbalance Modelling
## Add in sampling steps to recipes
```{r update workflows for different sampling techniques}
library(themis)

upsamp_rec<-insurance_recipe%>%
  step_upsample(Response,over_ratio = tune())

downsamp_rec<-insurance_recipe%>%
  step_downsample(Response,under_ratio = tune())

smote_rec<-insurance_recipe%>%
  step_smote(Response,over_ratio=tune(),neighbors = tune())


```


## Creating sampling tuning grids
```{r create sampling tuning grids}
upsamp_grid<-grid_regular(over_ratio()%>%range_set(c(.5,1.5)),
                          levels=11)
downsamp_grid<-grid_regular(under_ratio()%>%range_set(c(.5,1.5)),
                            levels=11)
smote_grid<-grid_regular(over_ratio()%>%range_set(c(.1,1.4)),
                         neighbors()%>%range_set(c(2,12)),
                         levels=14)

```

## Upsample model fit
```{r upsample model fit}
insurance_wf<-insurance_wf%>%
  update_recipe(upsamp_rec)%>%
  update_model(lr_mod)
set.seed(1111)
c1 <- makePSOCKcluster(5)
registerDoParallel(c1)

upsamp_lr_fit<-insurance_wf%>%
    tune_grid(resamples=cv_folds,
                  grid=upsamp_grid,
                      metrics = metric_set(
                      roc_auc,
                      spec,
                      sens,
                      f_meas),
                  control=control_grid(save_pred=TRUE,parallel_over = "resamples"))
stopCluster(c1)
saveRDS(upsamp_lr_fit,"upsamp_model.RDS")

```

## Downsample model fit
```{r downsample model fit}
set.seed(1234)
c1 <- makePSOCKcluster(5)
registerDoParallel(c1)

insurance_wf<-insurance_wf%>%update_recipe(downsamp_rec)
downsamp_lr_fit<-insurance_wf%>%
    tune_grid(resamples=cv_folds,
                  grid=downsamp_grid,
                      metrics = metric_set(
                      roc_auc,
                      spec,
                      sens,
                      f_meas),
                  control=control_grid(save_pred=TRUE,parallel_over = "resamples"))
stopCluster(c1)
saveRDS(downsamp_lr_fit,"downsamp_model.RDS")
```

## Smote model fit
```{r smote model fit}
set.seed(9876)
c1 <- makePSOCKcluster(5)
registerDoParallel(c1)
insurance_wf<-insurance_wf%>%update_recipe(smote_rec)
smote_lr_fit<-insurance_wf%>%
    tune_grid(resamples=cv_folds,
                  grid=smote_grid,
                      metrics = metric_set(
                      roc_auc,
                      spec,
                      sens,
                      f_meas),
                  control=control_grid(save_pred=TRUE,parallel_over = "resamples"))
stopCluster(c1)
saveRDS(smote_lr_fit,"smote_model.RDS")
```


```{r}
upsamp_lr_fit<-readRDS("upsamp_model.RDS")
downsamp_lr_fit<-readRDS("downsamp_model.RDS")
smote_lr_fit<-readRDS("smote_model.RDS")
```

```{r upsample plot}
upsamp_lr_fit %>%
  collect_metrics() %>%
  ggplot(aes(over_ratio, mean,color=mean)) +
  geom_point(size=2) +
  geom_line()+
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_color_viridis_c(begin =.9,end=.1)+
  labs(x="Over Ratio Parameter Values",
       title = "Upsample Logistic Regression")+
  theme(plot.title=element_text(hjust=.5))

```


```{r downsample plot}
downsamp_lr_fit %>%
  collect_metrics() %>%
  ggplot(aes(under_ratio, mean, color=mean)) +
  geom_point(size=2) +
  geom_line()+
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_color_viridis_c(begin =.9,end=.1)+
  labs(x="Under Ratio Parameter Values",
       title = "Downsample Logistic Regression")+
  theme(plot.title=element_text(hjust=.5))
```


```{r smote plot}
smote_lr_fit %>%
  collect_metrics() %>%
  mutate(neighbors = factor(neighbors)) %>%
  ggplot(aes(over_ratio, mean, color=neighbors)) +
  geom_line(alpha=.5) +
  geom_point(size=2,alpha=.5)+
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  labs(x="Over Ratio Parameter Values",
       title = "Smote Logistic Regression",
       color="Neighbors Parameter \nValues")+
  theme(plot.title=element_text(hjust=.5))
```


```{r selecting best models}
upsamp_best<-select_best(upsamp_lr_fit,metric='roc_auc')
downsamp_best<-select_best(downsamp_lr_fit,metric='roc_auc')
smote_best<-smote_lr_fit%>%
  show_best(metric='roc_auc',n=50)%>%
  arrange(neighbors)%>%
  slice(5)
```


```{r collect data for confusion matrices of sampled models}
upsamp_pred<-
    upsamp_lr_fit%>%
    collect_predictions(parameters=upsamp_best)

downsamp_pred<-
    downsamp_lr_fit%>%
    collect_predictions(parameters=downsamp_best)

smote_pred<-
    smote_lr_fit%>%
    collect_predictions(parameters=smote_best)
```

## Confusion Matrices
```{r}
cm_upsamp<-upsamp_pred%>%
    conf_mat(Response,.pred_class)%>%
    autoplot(type='heatmap')+
    ggtitle("Best Upsample")
cm_downsamp<-downsamp_pred%>%
    conf_mat(Response,.pred_class)%>%
    autoplot(type='heatmap')+
    ggtitle("Best Downsample")
cm_smote<-smote_pred%>%
    conf_mat(Response,.pred_class)%>%
    autoplot(type='heatmap')+
    ggtitle("Best Smote")
    
grid.arrange(cm_upsamp,cm_downsamp,cm_smote,ncol=3,heights=c(1,1))
```

## Test models with Validation set
```{r Fitting model to validation set}

upsamp_val_fit<-insurance_wf%>%
  update_recipe(upsamp_rec)%>%
  finalize_workflow(upsamp_best)%>%
  last_fit(split,metrics=metric_set(roc_auc,
           spec,
           sens))

downsamp_val_fit<-insurance_wf%>%
  update_recipe(downsamp_rec)%>%
  finalize_workflow(downsamp_best)%>%
  last_fit(split,metrics=metric_set(roc_auc,
           spec,
           sens))

smote_val_fit<-insurance_wf%>%
  update_recipe(smote_rec)%>%
  finalize_workflow(smote_best)%>%
  last_fit(split,metrics=metric_set(roc_auc,
           spec,
           sens))
```



## ROC curves
```{r gathering infor for ROC curves}
up_curve<-upsamp_val_fit%>%
  collect_predictions()%>%
  roc_curve(Response,.pred_No)%>%
  mutate(model="Best Upsample Model")

down_curve<-downsamp_val_fit%>%
  collect_predictions()%>%
  roc_curve(Response,.pred_No)%>%
  mutate(model="Best Downsample Model")

smote_curve<-smote_val_fit%>%
  collect_predictions()%>%
  roc_curve(Response,.pred_No)%>%
  mutate(model="Best Smote Model")
  
```

```{r }
  bind_rows(up_curve,down_curve,smote_curve)%>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```



```{r}

downsamp_final<-insurance_wf%>%
  update_recipe(downsamp_rec)%>%
  finalize_workflow(downsamp_best)%>%
  fit(train)

Response_pred<-predict(downsamp_final,new_data=test)%>%
  bind_cols(predict(downsamp_final,test,type='prob'))

```
